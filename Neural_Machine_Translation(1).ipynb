{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural Machine Translation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqA_XZnz5OWN",
        "colab_type": "text"
      },
      "source": [
        "## Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uoa5gidyWeVQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lines=[]\n",
        "with open('fra.txt') as f:\n",
        "  for line in f:\n",
        "    lines.append(line)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkUYQ9PlXg9Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lines=lines[:14000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PexUhZ0xX0m3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "english=[]\n",
        "french=[]\n",
        "for line in lines:\n",
        "  split_up=line.split('\\t')\n",
        "  english.append(split_up[0])\n",
        "  french.append(split_up[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-c-xoDl6YC0o",
        "colab_type": "code",
        "outputId": "f5e55528-fc26-4534-efb4-471364dd224d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "english[:3]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Go.', 'Hi.', 'Hi.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnWUb0ksYEKR",
        "colab_type": "code",
        "outputId": "22c8053f-aff7-4664-cd9c-237745db97b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "french[:3]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Va !', 'Salut !', 'Salut.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lx2QEHQAYfGf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_pairs=len(english)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQ4Zz4iPYl1H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#we want to use \\t as the start symbol and \\n as the stop symbol\n",
        "processed_french=[\"\\t\"+word+\"\\n\" for word in french]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3DreKYPdBBG",
        "colab_type": "code",
        "outputId": "5183ba11-1bc2-4141-bdf8-6db2dea10db8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "processed_french[:3]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\tVa !\\n', '\\tSalut !\\n', '\\tSalut.\\n']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4czBCCyidHnu",
        "colab_type": "code",
        "outputId": "1c9b56ec-3379-4ebe-9c40-64e1f2bdd14f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "english[:2]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Go.', 'Hi.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Q1TXz4KePdC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_characters=set()\n",
        "target_characters=set()\n",
        "\n",
        "for sequence in english:\n",
        "  for char in sequence:\n",
        "    if char not in input_characters:\n",
        "      input_characters.add(char)\n",
        "\n",
        "for sequence in processed_french:\n",
        "  for char in sequence:\n",
        "    if char not in target_characters:\n",
        "      target_characters.add(char)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnjAqnqcfgUR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_characters=sorted(list(input_characters))\n",
        "target_characters=sorted(list(target_characters))\n",
        "\n",
        "num_encoder_tokens=len(input_characters)\n",
        "num_decoder_tokens=len(target_characters)\n",
        "\n",
        "max_encoder_seq_length=max([len(s) for s in english])\n",
        "max_decoder_seq_length=max([len(s) for s in processed_french])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1IcOEPLgVjY",
        "colab_type": "code",
        "outputId": "ab2865e5-3ce3-4fcf-bb12-7780213f883a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "source": [
        "print('Number of samples: ', len(english))\n",
        "print('Number of unique input tokens: ',num_encoder_tokens )\n",
        "print('Number of unique target tokens: ',num_decoder_tokens )\n",
        "print('Max sequence length for inputs: ',max_encoder_seq_length)\n",
        "print('Max sequence length for targets: ',max_decoder_seq_length)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of samples:  14000\n",
            "Number of unique input tokens:  73\n",
            "Number of unique target tokens:  98\n",
            "Max sequence length for inputs:  17\n",
            "Max sequence length for targets:  59\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qmQ97szhcVg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_token_index={char:i for i,char in enumerate(input_characters)}\n",
        "target_token_index={char:i for i,char in enumerate(target_characters)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y48XoNk8huqX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "encoder_input_data=np.zeros((len(english),max_encoder_seq_length,num_encoder_tokens),dtype='float32')\n",
        "#one hot encoding english sentences by character"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dhorw5yAk8Rj",
        "colab_type": "code",
        "outputId": "5a4389b7-de1f-4028-e047-a2928339ac10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "len(english)==len(processed_french)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rp742gaCjHM5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder_input_data=np.zeros((len(processed_french),max_decoder_seq_length,num_decoder_tokens),dtype='float32')\n",
        "#one hot encoding french sentences for input into a decoder\n",
        "decoder_output_data=np.zeros((len(processed_french),max_decoder_seq_length,num_decoder_tokens),dtype='float32')\n",
        "#one hot encoding french sentences for their target data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGgNXd7Vle0z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i, (input_text,target_text) in enumerate(zip(english,processed_french)):\n",
        "  for t, char in enumerate(input_text):\n",
        "    encoder_input_data[i,t,input_token_index[char]] = 1 # filling up one hot encoded vector\n",
        "  \n",
        "  encoder_input_data[i,t+1:,input_token_index[' ']] = 1 #pad the sequences with spaces\n",
        "\n",
        "  for t,char in enumerate(target_text):\n",
        "    # decoder target data is ahead of decoder input data by one step and will not include the starting character\n",
        "    decoder_input_data[i,t,target_token_index[char]] = 1\n",
        "    if t > 0 :\n",
        "      decoder_output_data[i,t-1,target_token_index[char]] = 1 # the target sequence starts one step ahead\n",
        "    \n",
        "  decoder_input_data[i,t+1:,target_token_index[' ']] = 1 #pad up the decoder input sequences with spaces\n",
        "  decoder_output_data[i,t:,target_token_index[' ']] = 1 #pad up the decoder output sequences with spaces"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28XoITBipucl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "outputId": "e7c1a8fc-7d51-4378-c910-1d5461fef624"
      },
      "source": [
        "encoder_input_data[0] # the last few rows are padded by spaces"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [1., 0., 0., ..., 0., 0., 0.],\n",
              "       [1., 0., 0., ..., 0., 0., 0.],\n",
              "       [1., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tp0sHSfGqWBs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "affb68aa-d7d5-4e64-b036-ef5d53b1d1ab"
      },
      "source": [
        "decoder_input_data[0][1]"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeCRXsjwqYAm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "63b140b2-87b5-4c84-c625-07075b3df94e"
      },
      "source": [
        "decoder_output_data[0][0] # the first character of the target sequence is the second\n",
        "#character of in the decoder input sequence"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_-ZV9_eqqux",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Input, LSTM, Dense\n",
        "from tensorflow.keras.models import Model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cf8VtY5q5GLS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "latent_dim = 256 # dimensionality of encoding space\n",
        "epochs = 100\n",
        "num_samples=10000 # number of samples to use for training"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niNpA19l5J4A",
        "colab_type": "text"
      },
      "source": [
        "## Defining Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hw8pCivx2ZTJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Defining ENCODER for training process\n",
        "encoder_inputs=Input(shape=(None,num_encoder_tokens)) #RNNs can handle a sequence of any length\n",
        "encoder=LSTM(latent_dim,return_state=True) # return state will return a list \n",
        "# where the first entry is a list of of the outputs and the rest are internal states\n",
        "encoder_outputs, state_h, state_c=encoder(encoder_inputs)\n",
        "#discarding encoder outputs as we only need the hidden state and internal cell state\n",
        "encoder_states=[state_h,state_c] # hidden state and internal cell state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqsZqVWN6k4C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Defining DECODER for training process\n",
        "decoder_inputs=Input(shape=(None,num_decoder_tokens))\n",
        "decoder_lstm=LSTM(latent_dim,return_sequences=True,return_state=True)\n",
        "decoder_outputs, _, _=decoder_lstm(decoder_inputs,initial_state=encoder_states)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wz9T7eHN7DBW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder_dense=Dense(num_decoder_tokens,activation='softmax')\n",
        "decoder_outputs=decoder_dense(decoder_outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_PRO96J7WLl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model=Model([encoder_inputs,decoder_inputs],decoder_outputs) # this model is for learning mappings between\n",
        "#english and french sentences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7JTWc6p8E5I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "7f19d3bb-0433-4c51-acab-e256af9654d7"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, None, 73)]   0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            [(None, None, 98)]   0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     [(None, 256), (None, 337920      input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   [(None, None, 256),  363520      input_3[0][0]                    \n",
            "                                                                 lstm[0][1]                       \n",
            "                                                                 lstm[0][2]                       \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, None, 98)     25186       lstm_2[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 726,626\n",
            "Trainable params: 726,626\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-I2lWUpq9QdX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELwCVsYE97MK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5a1a2f4d-5439-473b-e877-7c93f45657cb"
      },
      "source": [
        "model.fit([encoder_input_data,decoder_input_data],decoder_output_data,epochs=epochs,validation_split=0.2)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "350/350 [==============================] - 54s 155ms/step - loss: 0.9959 - accuracy: 0.7468 - val_loss: 0.8539 - val_accuracy: 0.7540\n",
            "Epoch 2/100\n",
            "350/350 [==============================] - 58s 165ms/step - loss: 0.6457 - accuracy: 0.8132 - val_loss: 0.6911 - val_accuracy: 0.7973\n",
            "Epoch 3/100\n",
            "350/350 [==============================] - 65s 185ms/step - loss: 0.5441 - accuracy: 0.8398 - val_loss: 0.6148 - val_accuracy: 0.8184\n",
            "Epoch 4/100\n",
            "350/350 [==============================] - 63s 180ms/step - loss: 0.4899 - accuracy: 0.8543 - val_loss: 0.5748 - val_accuracy: 0.8289\n",
            "Epoch 5/100\n",
            "350/350 [==============================] - 64s 183ms/step - loss: 0.4490 - accuracy: 0.8661 - val_loss: 0.5352 - val_accuracy: 0.8395\n",
            "Epoch 6/100\n",
            "350/350 [==============================] - 62s 178ms/step - loss: 0.4170 - accuracy: 0.8751 - val_loss: 0.5108 - val_accuracy: 0.8489\n",
            "Epoch 7/100\n",
            "350/350 [==============================] - 66s 190ms/step - loss: 0.3900 - accuracy: 0.8833 - val_loss: 0.4943 - val_accuracy: 0.8541\n",
            "Epoch 8/100\n",
            "350/350 [==============================] - 63s 181ms/step - loss: 0.3670 - accuracy: 0.8899 - val_loss: 0.4782 - val_accuracy: 0.8588\n",
            "Epoch 9/100\n",
            "350/350 [==============================] - 62s 178ms/step - loss: 0.3466 - accuracy: 0.8960 - val_loss: 0.4662 - val_accuracy: 0.8626\n",
            "Epoch 10/100\n",
            "350/350 [==============================] - 62s 176ms/step - loss: 0.3285 - accuracy: 0.9015 - val_loss: 0.4570 - val_accuracy: 0.8656\n",
            "Epoch 11/100\n",
            "350/350 [==============================] - 64s 183ms/step - loss: 0.3118 - accuracy: 0.9061 - val_loss: 0.4489 - val_accuracy: 0.8686\n",
            "Epoch 12/100\n",
            "350/350 [==============================] - 66s 189ms/step - loss: 0.2971 - accuracy: 0.9106 - val_loss: 0.4438 - val_accuracy: 0.8703\n",
            "Epoch 13/100\n",
            "350/350 [==============================] - 66s 188ms/step - loss: 0.2833 - accuracy: 0.9142 - val_loss: 0.4439 - val_accuracy: 0.8714\n",
            "Epoch 14/100\n",
            "350/350 [==============================] - 63s 181ms/step - loss: 0.2709 - accuracy: 0.9180 - val_loss: 0.4414 - val_accuracy: 0.8725\n",
            "Epoch 15/100\n",
            "350/350 [==============================] - 65s 185ms/step - loss: 0.2591 - accuracy: 0.9213 - val_loss: 0.4451 - val_accuracy: 0.8724\n",
            "Epoch 16/100\n",
            "350/350 [==============================] - 64s 182ms/step - loss: 0.2486 - accuracy: 0.9243 - val_loss: 0.4440 - val_accuracy: 0.8739\n",
            "Epoch 17/100\n",
            "350/350 [==============================] - 67s 191ms/step - loss: 0.2383 - accuracy: 0.9274 - val_loss: 0.4435 - val_accuracy: 0.8745\n",
            "Epoch 18/100\n",
            "350/350 [==============================] - 70s 201ms/step - loss: 0.2289 - accuracy: 0.9301 - val_loss: 0.4477 - val_accuracy: 0.8741\n",
            "Epoch 19/100\n",
            "350/350 [==============================] - 68s 195ms/step - loss: 0.2204 - accuracy: 0.9325 - val_loss: 0.4476 - val_accuracy: 0.8752\n",
            "Epoch 20/100\n",
            "350/350 [==============================] - 67s 190ms/step - loss: 0.2117 - accuracy: 0.9352 - val_loss: 0.4534 - val_accuracy: 0.8749\n",
            "Epoch 21/100\n",
            "350/350 [==============================] - 70s 200ms/step - loss: 0.2039 - accuracy: 0.9374 - val_loss: 0.4557 - val_accuracy: 0.8745\n",
            "Epoch 22/100\n",
            "350/350 [==============================] - 70s 201ms/step - loss: 0.1960 - accuracy: 0.9397 - val_loss: 0.4599 - val_accuracy: 0.8755\n",
            "Epoch 23/100\n",
            "350/350 [==============================] - 68s 195ms/step - loss: 0.1895 - accuracy: 0.9415 - val_loss: 0.4665 - val_accuracy: 0.8750\n",
            "Epoch 24/100\n",
            "350/350 [==============================] - 65s 185ms/step - loss: 0.1825 - accuracy: 0.9440 - val_loss: 0.4728 - val_accuracy: 0.8751\n",
            "Epoch 25/100\n",
            "350/350 [==============================] - 67s 192ms/step - loss: 0.1762 - accuracy: 0.9455 - val_loss: 0.4752 - val_accuracy: 0.8755\n",
            "Epoch 26/100\n",
            "350/350 [==============================] - 72s 206ms/step - loss: 0.1701 - accuracy: 0.9472 - val_loss: 0.4828 - val_accuracy: 0.8749\n",
            "Epoch 27/100\n",
            "350/350 [==============================] - 69s 198ms/step - loss: 0.1645 - accuracy: 0.9489 - val_loss: 0.4867 - val_accuracy: 0.8741\n",
            "Epoch 28/100\n",
            "350/350 [==============================] - 68s 194ms/step - loss: 0.1588 - accuracy: 0.9506 - val_loss: 0.4921 - val_accuracy: 0.8747\n",
            "Epoch 29/100\n",
            "350/350 [==============================] - 67s 192ms/step - loss: 0.1539 - accuracy: 0.9520 - val_loss: 0.5022 - val_accuracy: 0.8738\n",
            "Epoch 30/100\n",
            "350/350 [==============================] - 65s 186ms/step - loss: 0.1491 - accuracy: 0.9535 - val_loss: 0.5051 - val_accuracy: 0.8744\n",
            "Epoch 31/100\n",
            "350/350 [==============================] - 71s 204ms/step - loss: 0.1444 - accuracy: 0.9549 - val_loss: 0.5094 - val_accuracy: 0.8742\n",
            "Epoch 32/100\n",
            "350/350 [==============================] - 71s 202ms/step - loss: 0.1401 - accuracy: 0.9561 - val_loss: 0.5151 - val_accuracy: 0.8745\n",
            "Epoch 33/100\n",
            "350/350 [==============================] - 68s 195ms/step - loss: 0.1358 - accuracy: 0.9575 - val_loss: 0.5200 - val_accuracy: 0.8741\n",
            "Epoch 34/100\n",
            "350/350 [==============================] - 70s 200ms/step - loss: 0.1317 - accuracy: 0.9584 - val_loss: 0.5253 - val_accuracy: 0.8747\n",
            "Epoch 35/100\n",
            "350/350 [==============================] - 71s 203ms/step - loss: 0.1277 - accuracy: 0.9598 - val_loss: 0.5342 - val_accuracy: 0.8734\n",
            "Epoch 36/100\n",
            "350/350 [==============================] - 73s 209ms/step - loss: 0.1244 - accuracy: 0.9609 - val_loss: 0.5415 - val_accuracy: 0.8739\n",
            "Epoch 37/100\n",
            "350/350 [==============================] - 67s 191ms/step - loss: 0.1209 - accuracy: 0.9619 - val_loss: 0.5433 - val_accuracy: 0.8734\n",
            "Epoch 38/100\n",
            "350/350 [==============================] - 68s 195ms/step - loss: 0.1173 - accuracy: 0.9631 - val_loss: 0.5526 - val_accuracy: 0.8728\n",
            "Epoch 39/100\n",
            "350/350 [==============================] - 69s 198ms/step - loss: 0.1146 - accuracy: 0.9638 - val_loss: 0.5565 - val_accuracy: 0.8736\n",
            "Epoch 40/100\n",
            "350/350 [==============================] - 71s 203ms/step - loss: 0.1113 - accuracy: 0.9647 - val_loss: 0.5643 - val_accuracy: 0.8721\n",
            "Epoch 41/100\n",
            "350/350 [==============================] - 68s 194ms/step - loss: 0.1083 - accuracy: 0.9656 - val_loss: 0.5692 - val_accuracy: 0.8725\n",
            "Epoch 42/100\n",
            "350/350 [==============================] - 67s 193ms/step - loss: 0.1057 - accuracy: 0.9664 - val_loss: 0.5720 - val_accuracy: 0.8724\n",
            "Epoch 43/100\n",
            "350/350 [==============================] - 72s 206ms/step - loss: 0.1029 - accuracy: 0.9671 - val_loss: 0.5795 - val_accuracy: 0.8725\n",
            "Epoch 44/100\n",
            "350/350 [==============================] - 71s 203ms/step - loss: 0.1002 - accuracy: 0.9679 - val_loss: 0.5849 - val_accuracy: 0.8725\n",
            "Epoch 45/100\n",
            "350/350 [==============================] - 68s 195ms/step - loss: 0.0981 - accuracy: 0.9686 - val_loss: 0.5882 - val_accuracy: 0.8725\n",
            "Epoch 46/100\n",
            "350/350 [==============================] - 70s 201ms/step - loss: 0.0955 - accuracy: 0.9696 - val_loss: 0.5956 - val_accuracy: 0.8727\n",
            "Epoch 47/100\n",
            "350/350 [==============================] - 66s 189ms/step - loss: 0.0931 - accuracy: 0.9702 - val_loss: 0.6014 - val_accuracy: 0.8717\n",
            "Epoch 48/100\n",
            "350/350 [==============================] - 66s 189ms/step - loss: 0.0911 - accuracy: 0.9708 - val_loss: 0.6013 - val_accuracy: 0.8720\n",
            "Epoch 49/100\n",
            "350/350 [==============================] - 70s 201ms/step - loss: 0.0885 - accuracy: 0.9716 - val_loss: 0.6118 - val_accuracy: 0.8719\n",
            "Epoch 50/100\n",
            "350/350 [==============================] - 67s 192ms/step - loss: 0.0871 - accuracy: 0.9719 - val_loss: 0.6139 - val_accuracy: 0.8718\n",
            "Epoch 51/100\n",
            "350/350 [==============================] - 67s 192ms/step - loss: 0.0847 - accuracy: 0.9724 - val_loss: 0.6256 - val_accuracy: 0.8716\n",
            "Epoch 52/100\n",
            "350/350 [==============================] - 67s 191ms/step - loss: 0.0830 - accuracy: 0.9731 - val_loss: 0.6307 - val_accuracy: 0.8715\n",
            "Epoch 53/100\n",
            "350/350 [==============================] - 70s 200ms/step - loss: 0.0813 - accuracy: 0.9735 - val_loss: 0.6319 - val_accuracy: 0.8711\n",
            "Epoch 54/100\n",
            "350/350 [==============================] - 66s 188ms/step - loss: 0.0796 - accuracy: 0.9739 - val_loss: 0.6387 - val_accuracy: 0.8717\n",
            "Epoch 55/100\n",
            "350/350 [==============================] - 64s 182ms/step - loss: 0.0778 - accuracy: 0.9747 - val_loss: 0.6418 - val_accuracy: 0.8714\n",
            "Epoch 56/100\n",
            "350/350 [==============================] - 65s 186ms/step - loss: 0.0761 - accuracy: 0.9752 - val_loss: 0.6476 - val_accuracy: 0.8709\n",
            "Epoch 57/100\n",
            "350/350 [==============================] - 67s 191ms/step - loss: 0.0747 - accuracy: 0.9756 - val_loss: 0.6462 - val_accuracy: 0.8721\n",
            "Epoch 58/100\n",
            "350/350 [==============================] - 69s 197ms/step - loss: 0.0735 - accuracy: 0.9757 - val_loss: 0.6556 - val_accuracy: 0.8712\n",
            "Epoch 59/100\n",
            "350/350 [==============================] - 67s 192ms/step - loss: 0.0717 - accuracy: 0.9763 - val_loss: 0.6594 - val_accuracy: 0.8714\n",
            "Epoch 60/100\n",
            "350/350 [==============================] - 68s 193ms/step - loss: 0.0704 - accuracy: 0.9768 - val_loss: 0.6604 - val_accuracy: 0.8718\n",
            "Epoch 61/100\n",
            "350/350 [==============================] - 65s 185ms/step - loss: 0.0688 - accuracy: 0.9774 - val_loss: 0.6673 - val_accuracy: 0.8710\n",
            "Epoch 62/100\n",
            "350/350 [==============================] - 64s 182ms/step - loss: 0.0680 - accuracy: 0.9772 - val_loss: 0.6720 - val_accuracy: 0.8710\n",
            "Epoch 63/100\n",
            "350/350 [==============================] - 65s 184ms/step - loss: 0.0664 - accuracy: 0.9779 - val_loss: 0.6806 - val_accuracy: 0.8710\n",
            "Epoch 64/100\n",
            "350/350 [==============================] - 66s 188ms/step - loss: 0.0655 - accuracy: 0.9784 - val_loss: 0.6809 - val_accuracy: 0.8710\n",
            "Epoch 65/100\n",
            "350/350 [==============================] - 69s 197ms/step - loss: 0.0641 - accuracy: 0.9788 - val_loss: 0.6868 - val_accuracy: 0.8707\n",
            "Epoch 66/100\n",
            "350/350 [==============================] - 67s 192ms/step - loss: 0.0632 - accuracy: 0.9786 - val_loss: 0.6929 - val_accuracy: 0.8702\n",
            "Epoch 67/100\n",
            "350/350 [==============================] - 69s 197ms/step - loss: 0.0619 - accuracy: 0.9792 - val_loss: 0.6987 - val_accuracy: 0.8710\n",
            "Epoch 68/100\n",
            "350/350 [==============================] - 68s 193ms/step - loss: 0.0613 - accuracy: 0.9794 - val_loss: 0.7034 - val_accuracy: 0.8708\n",
            "Epoch 69/100\n",
            "350/350 [==============================] - 64s 184ms/step - loss: 0.0602 - accuracy: 0.9799 - val_loss: 0.7033 - val_accuracy: 0.8703\n",
            "Epoch 70/100\n",
            "350/350 [==============================] - 62s 177ms/step - loss: 0.0592 - accuracy: 0.9801 - val_loss: 0.7091 - val_accuracy: 0.8699\n",
            "Epoch 71/100\n",
            "350/350 [==============================] - 65s 186ms/step - loss: 0.0583 - accuracy: 0.9803 - val_loss: 0.7056 - val_accuracy: 0.8707\n",
            "Epoch 72/100\n",
            "350/350 [==============================] - 66s 188ms/step - loss: 0.0573 - accuracy: 0.9806 - val_loss: 0.7182 - val_accuracy: 0.8702\n",
            "Epoch 73/100\n",
            "350/350 [==============================] - 65s 187ms/step - loss: 0.0563 - accuracy: 0.9809 - val_loss: 0.7191 - val_accuracy: 0.8702\n",
            "Epoch 74/100\n",
            "350/350 [==============================] - 67s 190ms/step - loss: 0.0555 - accuracy: 0.9811 - val_loss: 0.7226 - val_accuracy: 0.8698\n",
            "Epoch 75/100\n",
            "350/350 [==============================] - 67s 190ms/step - loss: 0.0549 - accuracy: 0.9811 - val_loss: 0.7250 - val_accuracy: 0.8699\n",
            "Epoch 76/100\n",
            "350/350 [==============================] - 67s 192ms/step - loss: 0.0542 - accuracy: 0.9813 - val_loss: 0.7222 - val_accuracy: 0.8699\n",
            "Epoch 77/100\n",
            "350/350 [==============================] - 69s 197ms/step - loss: 0.0537 - accuracy: 0.9815 - val_loss: 0.7330 - val_accuracy: 0.8706\n",
            "Epoch 78/100\n",
            "350/350 [==============================] - 69s 197ms/step - loss: 0.0528 - accuracy: 0.9818 - val_loss: 0.7380 - val_accuracy: 0.8698\n",
            "Epoch 79/100\n",
            "350/350 [==============================] - 64s 182ms/step - loss: 0.0520 - accuracy: 0.9822 - val_loss: 0.7402 - val_accuracy: 0.8702\n",
            "Epoch 80/100\n",
            "350/350 [==============================] - 67s 191ms/step - loss: 0.0519 - accuracy: 0.9820 - val_loss: 0.7407 - val_accuracy: 0.8705\n",
            "Epoch 81/100\n",
            "350/350 [==============================] - 65s 187ms/step - loss: 0.0512 - accuracy: 0.9825 - val_loss: 0.7487 - val_accuracy: 0.8697\n",
            "Epoch 82/100\n",
            "350/350 [==============================] - 67s 192ms/step - loss: 0.0503 - accuracy: 0.9825 - val_loss: 0.7512 - val_accuracy: 0.8700\n",
            "Epoch 83/100\n",
            "350/350 [==============================] - 64s 184ms/step - loss: 0.0498 - accuracy: 0.9828 - val_loss: 0.7504 - val_accuracy: 0.8707\n",
            "Epoch 84/100\n",
            "350/350 [==============================] - 65s 185ms/step - loss: 0.0488 - accuracy: 0.9831 - val_loss: 0.7517 - val_accuracy: 0.8697\n",
            "Epoch 85/100\n",
            "350/350 [==============================] - 66s 188ms/step - loss: 0.0487 - accuracy: 0.9830 - val_loss: 0.7598 - val_accuracy: 0.8692\n",
            "Epoch 86/100\n",
            "350/350 [==============================] - 66s 188ms/step - loss: 0.0478 - accuracy: 0.9833 - val_loss: 0.7597 - val_accuracy: 0.8689\n",
            "Epoch 87/100\n",
            "350/350 [==============================] - 68s 194ms/step - loss: 0.0477 - accuracy: 0.9833 - val_loss: 0.7626 - val_accuracy: 0.8697\n",
            "Epoch 88/100\n",
            "350/350 [==============================] - 66s 189ms/step - loss: 0.0475 - accuracy: 0.9834 - val_loss: 0.7639 - val_accuracy: 0.8704\n",
            "Epoch 89/100\n",
            "350/350 [==============================] - 65s 187ms/step - loss: 0.0470 - accuracy: 0.9836 - val_loss: 0.7624 - val_accuracy: 0.8702\n",
            "Epoch 90/100\n",
            "350/350 [==============================] - 70s 199ms/step - loss: 0.0458 - accuracy: 0.9839 - val_loss: 0.7709 - val_accuracy: 0.8691\n",
            "Epoch 91/100\n",
            "350/350 [==============================] - 68s 195ms/step - loss: 0.0459 - accuracy: 0.9840 - val_loss: 0.7732 - val_accuracy: 0.8700\n",
            "Epoch 92/100\n",
            "350/350 [==============================] - 66s 188ms/step - loss: 0.0452 - accuracy: 0.9841 - val_loss: 0.7769 - val_accuracy: 0.8699\n",
            "Epoch 93/100\n",
            "350/350 [==============================] - 68s 194ms/step - loss: 0.0448 - accuracy: 0.9842 - val_loss: 0.7789 - val_accuracy: 0.8700\n",
            "Epoch 94/100\n",
            "350/350 [==============================] - 63s 180ms/step - loss: 0.0445 - accuracy: 0.9843 - val_loss: 0.7822 - val_accuracy: 0.8696\n",
            "Epoch 95/100\n",
            "350/350 [==============================] - 68s 194ms/step - loss: 0.0440 - accuracy: 0.9845 - val_loss: 0.7819 - val_accuracy: 0.8703\n",
            "Epoch 96/100\n",
            "350/350 [==============================] - 69s 196ms/step - loss: 0.0440 - accuracy: 0.9843 - val_loss: 0.7824 - val_accuracy: 0.8701\n",
            "Epoch 97/100\n",
            "350/350 [==============================] - 68s 194ms/step - loss: 0.0433 - accuracy: 0.9848 - val_loss: 0.7901 - val_accuracy: 0.8696\n",
            "Epoch 98/100\n",
            "350/350 [==============================] - 65s 184ms/step - loss: 0.0429 - accuracy: 0.9848 - val_loss: 0.7874 - val_accuracy: 0.8698\n",
            "Epoch 99/100\n",
            "350/350 [==============================] - 68s 195ms/step - loss: 0.0428 - accuracy: 0.9847 - val_loss: 0.7982 - val_accuracy: 0.8694\n",
            "Epoch 100/100\n",
            "350/350 [==============================] - 69s 198ms/step - loss: 0.0422 - accuracy: 0.9851 - val_loss: 0.7965 - val_accuracy: 0.8698\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1e89c61ac8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hS-SqC2-L2L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('s2s.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zb2HWuRiGfqS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder_model=Model(encoder_inputs,encoder_states)# will take a one hot encoded english sequence as input\n",
        "# and will output hidden state and cell state\n",
        "#shape=(None,num_encoder_tokens)\n",
        "#this model is for encoding some input sequence into state values\n",
        "#the encoder_model is using layers from the trained model\n",
        "\n",
        "decoder_state_input_h=Input(shape=(latent_dim,)) #dimensions of the hidden state are same as that\n",
        "# of encoding_space\n",
        "decoder_state_input_c=Input(shape=(latent_dim,))\n",
        "\n",
        "decoder_states_inputs=[decoder_state_input_h, decoder_state_input_c] # these states are passed in from the encoder\n",
        "#model\n",
        "\n",
        "decoder_outputs, state_h, state_c=decoder_lstm(decoder_inputs,initial_state=decoder_states_inputs)\n",
        "\n",
        "decoder_states=[state_h,state_c]\n",
        "#these states are the output states after the encoder states and the one hot encoded target sequence has been passed through the\n",
        "#decoder LSTM layer which returns sequences\n",
        "decoder_outputs=decoder_dense(decoder_outputs)\n",
        "#this is the softmax output for predicting the next character based on the context passed in by the encoder\n",
        "decoder_model=Model([decoder_inputs]+decoder_states_inputs,[decoder_outputs]+decoder_states)# the decoder model\n",
        "#decoder_model is also using layers that have weight attached to them from a trained model\n",
        "#reverse look up for english and french characters\n",
        "reverse_input_char_index={char:i for i,char in input_token_index.items()}\n",
        "reverse_target_char_index={char:i for i,char in target_token_index.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5GirELxZS36",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reverse_input_char_index={char:i for i,char in input_token_index.items()}\n",
        "reverse_target_char_index={char:i for i,char in target_token_index.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_FsY2fjKMOB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decode_sequence(input_seq):\n",
        "  #encode input into states\n",
        "  states_value = encoder_model.predict(input_seq) # forward pass through encoder model to get states\n",
        "\n",
        "  # generate an empty target sequence of length 1, so a single one hot encoded character with french characters characters\n",
        "  target_seq=np.zeros((1,1,num_decoder_tokens))\n",
        "  #set the first character of the target sequence as the start symbol '\\t'\n",
        "  target_seq[0,0,target_token_index['\\t']]=1\n",
        "\n",
        "  stop_condition=False\n",
        "  decoded_sentence=''\n",
        "\n",
        "  while not stop_condition:\n",
        "    output_tokens, h, c =decoder_model.predict([target_seq]+states_value) # the target sequence will\n",
        "    #first start of with the start symbol and the hidden and cell state of the encoded english input.\n",
        "    #which will then output tokens and hidden and cell states\n",
        "    sampled_token_index=np.argmax(output_tokens[0,-1,:]) # index of the character that the model has\n",
        "    #predicted to be the next character\n",
        "    sampled_char=reverse_target_char_index[sampled_token_index]\n",
        "    decoded_sentence += sampled_char\n",
        "\n",
        "    if sampled_char=='\\n' or len(decoded_sentence)> max_decoder_seq_length:\n",
        "      #stop generating as soon as model predicts a stop symbol or the sequence length exceeds the maximum length of\n",
        "      #french sequences\n",
        "      stop_condition=True\n",
        "    \n",
        "    #again we'll generate an empty target sequence for one character that is one hot encoded with the\n",
        "    #last predicted character\n",
        "    target_seq=np.zeros((1,1,num_decoder_tokens))\n",
        "    target_seq[0,0,sampled_token_index]=1\n",
        "\n",
        "    states_value=[h,c] # update the input states\n",
        "  \n",
        "  return decoded_sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eun48h6MX3NG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "outputId": "177b8dc9-5156-46f9-c211-d29cd082d18b"
      },
      "source": [
        "encoder_input_data[0:1]"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [1., 0., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 0., ..., 0., 0., 0.]]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tgajiF_YO7d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "58382a11-4d93-4279-da13-1ffbab540f36"
      },
      "source": [
        "for seq_index in range(50):\n",
        "  print('*'*10)\n",
        "  input_seq=encoder_input_data[seq_index:seq_index+1]# to add an extra dimension to the encoded sequence\n",
        "  decoded_sentence=decode_sequence(input_seq)\n",
        "  print('Input Sentence: ',english[seq_index])\n",
        "  print('Decoded Sequence: ', decoded_sentence)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "**********\n",
            "Input Sentence:  Go.\n",
            "Decoded Sequence:  Va !\n",
            "\n",
            "**********\n",
            "Input Sentence:  Hi.\n",
            "Decoded Sequence:  Salut !\n",
            "\n",
            "**********\n",
            "Input Sentence:  Hi.\n",
            "Decoded Sequence:  Salut !\n",
            "\n",
            "**********\n",
            "Input Sentence:  Run!\n",
            "Decoded Sequence:  Cours !\n",
            "\n",
            "**********\n",
            "Input Sentence:  Run!\n",
            "Decoded Sequence:  Cours !\n",
            "\n",
            "**********\n",
            "Input Sentence:  Who?\n",
            "Decoded Sequence:  Qui ?\n",
            "\n",
            "**********\n",
            "Input Sentence:  Wow!\n",
            "Decoded Sequence:  Ça alors !\n",
            "\n",
            "**********\n",
            "Input Sentence:  Fire!\n",
            "Decoded Sequence:  Au feu !\n",
            "\n",
            "**********\n",
            "Input Sentence:  Help!\n",
            "Decoded Sequence:  À l'aide !\n",
            "\n",
            "**********\n",
            "Input Sentence:  Jump.\n",
            "Decoded Sequence:  Saute.\n",
            "\n",
            "**********\n",
            "Input Sentence:  Stop!\n",
            "Decoded Sequence:  Stop !\n",
            "\n",
            "**********\n",
            "Input Sentence:  Stop!\n",
            "Decoded Sequence:  Stop !\n",
            "\n",
            "**********\n",
            "Input Sentence:  Stop!\n",
            "Decoded Sequence:  Stop !\n",
            "\n",
            "**********\n",
            "Input Sentence:  Wait!\n",
            "Decoded Sequence:  Attends !\n",
            "\n",
            "**********\n",
            "Input Sentence:  Wait!\n",
            "Decoded Sequence:  Attends !\n",
            "\n",
            "**********\n",
            "Input Sentence:  Go on.\n",
            "Decoded Sequence:  Poursuis.\n",
            "\n",
            "**********\n",
            "Input Sentence:  Go on.\n",
            "Decoded Sequence:  Poursuis.\n",
            "\n",
            "**********\n",
            "Input Sentence:  Go on.\n",
            "Decoded Sequence:  Poursuis.\n",
            "\n",
            "**********\n",
            "Input Sentence:  Hello!\n",
            "Decoded Sequence:  Bonjour !\n",
            "\n",
            "**********\n",
            "Input Sentence:  Hello!\n",
            "Decoded Sequence:  Bonjour !\n",
            "\n",
            "**********\n",
            "Input Sentence:  I see.\n",
            "Decoded Sequence:  Je comprends.\n",
            "\n",
            "**********\n",
            "Input Sentence:  I try.\n",
            "Decoded Sequence:  J'essaye.\n",
            "\n",
            "**********\n",
            "Input Sentence:  I won!\n",
            "Decoded Sequence:  Je l'ai emporté !\n",
            "\n",
            "**********\n",
            "Input Sentence:  I won!\n",
            "Decoded Sequence:  Je l'ai emporté !\n",
            "\n",
            "**********\n",
            "Input Sentence:  I won.\n",
            "Decoded Sequence:  J’ai gagné.\n",
            "\n",
            "**********\n",
            "Input Sentence:  Oh no!\n",
            "Decoded Sequence:  Oh non !\n",
            "\n",
            "**********\n",
            "Input Sentence:  Attack!\n",
            "Decoded Sequence:  Attaque !\n",
            "\n",
            "**********\n",
            "Input Sentence:  Attack!\n",
            "Decoded Sequence:  Attaque !\n",
            "\n",
            "**********\n",
            "Input Sentence:  Cheers!\n",
            "Decoded Sequence:  Santé !\n",
            "\n",
            "**********\n",
            "Input Sentence:  Cheers!\n",
            "Decoded Sequence:  Santé !\n",
            "\n",
            "**********\n",
            "Input Sentence:  Cheers!\n",
            "Decoded Sequence:  Santé !\n",
            "\n",
            "**********\n",
            "Input Sentence:  Cheers!\n",
            "Decoded Sequence:  Santé !\n",
            "\n",
            "**********\n",
            "Input Sentence:  Get up.\n",
            "Decoded Sequence:  Lève-toi.\n",
            "\n",
            "**********\n",
            "Input Sentence:  Go now.\n",
            "Decoded Sequence:  Vas-y maintenant.\n",
            "\n",
            "**********\n",
            "Input Sentence:  Go now.\n",
            "Decoded Sequence:  Vas-y maintenant.\n",
            "\n",
            "**********\n",
            "Input Sentence:  Go now.\n",
            "Decoded Sequence:  Vas-y maintenant.\n",
            "\n",
            "**********\n",
            "Input Sentence:  Got it!\n",
            "Decoded Sequence:  Compris !\n",
            "\n",
            "**********\n",
            "Input Sentence:  Got it!\n",
            "Decoded Sequence:  Compris !\n",
            "\n",
            "**********\n",
            "Input Sentence:  Got it?\n",
            "Decoded Sequence:  Pigé ?\n",
            "\n",
            "**********\n",
            "Input Sentence:  Got it?\n",
            "Decoded Sequence:  Pigé ?\n",
            "\n",
            "**********\n",
            "Input Sentence:  Got it?\n",
            "Decoded Sequence:  Pigé ?\n",
            "\n",
            "**********\n",
            "Input Sentence:  Hop in.\n",
            "Decoded Sequence:  Monte.\n",
            "\n",
            "**********\n",
            "Input Sentence:  Hop in.\n",
            "Decoded Sequence:  Monte.\n",
            "\n",
            "**********\n",
            "Input Sentence:  Hug me.\n",
            "Decoded Sequence:  Serre-moi dans tes bras !\n",
            "\n",
            "**********\n",
            "Input Sentence:  Hug me.\n",
            "Decoded Sequence:  Serre-moi dans tes bras !\n",
            "\n",
            "**********\n",
            "Input Sentence:  I fell.\n",
            "Decoded Sequence:  Je suis tombée.\n",
            "\n",
            "**********\n",
            "Input Sentence:  I fell.\n",
            "Decoded Sequence:  Je suis tombée.\n",
            "\n",
            "**********\n",
            "Input Sentence:  I know.\n",
            "Decoded Sequence:  Je sais.\n",
            "\n",
            "**********\n",
            "Input Sentence:  I left.\n",
            "Decoded Sequence:  Je suis parti.\n",
            "\n",
            "**********\n",
            "Input Sentence:  I left.\n",
            "Decoded Sequence:  Je suis parti.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}